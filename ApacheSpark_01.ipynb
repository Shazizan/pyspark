{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8TOPGbjNwgoo",
        "s8-sZgW9wgpG",
        "iI4H_G6awgpK",
        "WqMwL6z6wgpL",
        "2-RTITA4wgpN",
        "gBw_qei6wgpQ",
        "xgTbMjgAwgpU",
        "gKR7y9vHwgpY",
        "ZQEl4pQfwgpe",
        "v7CSwYEvwgph"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shazizan/pyspark/blob/main/ApacheSpark_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v48WoZI6wgoa"
      },
      "source": [
        "<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%\" align=right>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuyBPUSswgod"
      },
      "source": [
        "# Apache Spark DataFrames and Spark SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr53TGM_wgoe"
      },
      "source": [
        "Apache Spark is a platform for distributed data processing, and it is particularly well-suited for dealing with massive data sets. The data sets that they do not readily fit within the memory or capacity of a single server.\n",
        "\n",
        "Apache Spark has a modular architecture. A core platform is called Apache Spark core, and there are several modules, which run on top of the core platform.\n",
        "\n",
        "In this notebook, we will mostly learn about DataFrames and work with Spark SQL. Apache Spark supports multiple languages, including:\n",
        "- Scala\n",
        "- Python\n",
        "- Java\n",
        "- Python\n",
        "- R\n",
        "\n",
        "Apache Spark's core data structure is the Resilient Distributed Dataset (RDD). RDD is a low-level object that lets Spark work by splitting data across multiple nodes in the cluster. However, working directly with RDDs is hard. Therefore, data scientists and data engineers prefer to use the Spark DataFrame abstraction built on top of RDDs.\n",
        "\n",
        "We are particularly interested in a data structure called DataFrames. DataFrames are a set of data that are organized into columns and rows. The columns have names, and the rows have a schema. Therefore, in this way, they are very similar or analogous to tables in relational databases. Not only DataFrames are easier to understand, but also they are more optimized for complicated operations than RDDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZftkIg9Ywgof"
      },
      "source": [
        "There are a couple of different ways of working with DataFrames. One way is to use the DataFrame API, and basically, that is structured around using methods on DataFrame objects. The second way is Spark SQL that allows us to enter SQL queries which are executed on DataFrames, and those DataFrames are registered as tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZL6TPUewgog"
      },
      "source": [
        "### Setup Apache Spark on Jupyter\n",
        "To start working with DataFrames, first of all, we have to create a `SparkSession` object from `SparkContext`. The `SparkContext` is a connection to the running cluster, and `SparkSession` is an interface with that connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RXuPrdVwgoh",
        "outputId": "93830833-9b01-49a5-cd19-dcf70925802e"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=d4adf735b7a538321915001090e2269645f8c69c85872afcfdc1537d5bbbebec\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfWR9i6pwgoh"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_1iNTn2wgoi"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl_Th8Svwgoi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKF7iqyvwgoi"
      },
      "source": [
        "The previous line of code returns an existing `SparkSession` if there's already one in the environment, or creates a new one if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "WWdcWWBmwgoj",
        "outputId": "8373f261-0001-47fe-95f1-85c68faca73d"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://fd7c99c9a504:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd66f118a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKhIjsL0wgoj"
      },
      "source": [
        "### Make the data set folder accessible\n",
        "\n",
        "In the following cells, we are going to load a file called `location_temp.csv`, which is a time-series file which contains loacations of sensors and the temperatures taken at particular periods of time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kyz1nP52wgok",
        "outputId": "b3db422e-b520-4daf-868c-599d7d1dc34c"
      },
      "source": [
        "import os\n",
        "MAIN_DIRECTORY = os.getcwd()\n",
        "MAIN_DIRECTORY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8FZFtVdwgok"
      },
      "source": [
        "file_path = MAIN_DIRECTORY +\"/data/location_temp.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xw28T1RU4pUM",
        "outputId": "7a74a423-f0c1-413d-e2c2-19f8795dce0e"
      },
      "source": [
        "file_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/data/location_temp.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bstmwwuwgok"
      },
      "source": [
        "## Get Started with Spark DataFrames\n",
        "To create a Spark DataFrame by loading a csv file, we can use `spark.read` function as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSoce-GAwgol"
      },
      "source": [
        "df1 = spark.read.format('csv').option('header','true').load(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoz6WoiB5TUJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0To1t3wgol"
      },
      "source": [
        "We can use `head(n)` method to show the heading of this data frame. `n` is the number of rows and its default value is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbJxbzRVwgol",
        "outputId": "6fe5b56f-182d-4fca-b9d0-cb9f87f5f8dd"
      },
      "source": [
        "df1.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n",
              " Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n",
              " Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n",
              " Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n",
              " Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj6vNaXnwgom"
      },
      "source": [
        "If we want to show the data in a tabular format, we can use `.show(n)` method. `n` is the number of rows and its default value is 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFBViXDhwgom",
        "outputId": "053b2ea9-109e-4455-9fe5-80aead7596e9"
      },
      "source": [
        "df1.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+------------+\n",
            "|         event_date|location_id|temp_celcius|\n",
            "+-------------------+-----------+------------+\n",
            "|03/04/2019 19:48:06|       loc0|          29|\n",
            "|03/04/2019 19:53:06|       loc0|          27|\n",
            "|03/04/2019 19:58:06|       loc0|          28|\n",
            "|03/04/2019 20:03:06|       loc0|          30|\n",
            "|03/04/2019 20:08:06|       loc0|          27|\n",
            "|03/04/2019 20:13:06|       loc0|          27|\n",
            "|03/04/2019 20:18:06|       loc0|          27|\n",
            "|03/04/2019 20:23:06|       loc0|          29|\n",
            "|03/04/2019 20:28:06|       loc0|          32|\n",
            "|03/04/2019 20:33:06|       loc0|          35|\n",
            "+-------------------+-----------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODE3nKfwgom"
      },
      "source": [
        "To know the number of rows in the DataFrame, there is a useful method called `count()` that performs a count on the rows in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGXdfn5Ywgom",
        "outputId": "33d4fb8f-cfc6-4844-84a0-8ab4baea5388"
      },
      "source": [
        "df1.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG3ojJOKwgon"
      },
      "source": [
        "One of the useful methods in DataFrame API is `printSchema()` that prints out the schema in the tree format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JVhwlh0wgon",
        "outputId": "5dae8712-3dc6-40ab-e1d5-3ebb44a0bed8"
      },
      "source": [
        "df1.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- event_date: string (nullable = true)\n",
            " |-- location_id: string (nullable = true)\n",
            " |-- temp_celcius: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TOPGbjNwgoo"
      },
      "source": [
        "### Rename column names\n",
        "\n",
        "Now, let's load another file. In the data folder, we have another file called `utilization.csv`. This file does not have a header row. If we want to use the csv file schema, Spark provides an option to infer the columns' data types automatically. The following cells show how we can work with this type of csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjmrJMqdwgoo"
      },
      "source": [
        "file_path = MAIN_DIRECTORY +\"/data/utilization.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZOxTpgwgop"
      },
      "source": [
        "df2 = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkFburAwwgop",
        "outputId": "99bba0a9-1b20-491b-9066-53acb9f1f9b8"
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLwkyhx_wgoq",
        "outputId": "91cab098-3a15-4a86-84d2-26b6fd9b5071"
      },
      "source": [
        "df2.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0='03/05/2019 08:06:14', _c1=100, _c2=0.57, _c3=0.51, _c4=47),\n",
              " Row(_c0='03/05/2019 08:11:14', _c1=100, _c2=0.47, _c3=0.62, _c4=43),\n",
              " Row(_c0='03/05/2019 08:16:14', _c1=100, _c2=0.56, _c3=0.57, _c4=62),\n",
              " Row(_c0='03/05/2019 08:21:14', _c1=100, _c2=0.57, _c3=0.56, _c4=50),\n",
              " Row(_c0='03/05/2019 08:26:14', _c1=100, _c2=0.35, _c3=0.46, _c4=43)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlh3c53K8A84",
        "outputId": "6c5f56ae-1c2a-48b9-c5e2-20c3a7ca2172"
      },
      "source": [
        "df2.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---+----+----+---+\n",
            "|                _c0|_c1| _c2| _c3|_c4|\n",
            "+-------------------+---+----+----+---+\n",
            "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
            "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
            "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
            "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
            "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
            "|03/05/2019 08:31:14|100|0.41|0.58| 48|\n",
            "|03/05/2019 08:36:14|100|0.57|0.35| 58|\n",
            "|03/05/2019 08:41:14|100|0.41| 0.4| 58|\n",
            "|03/05/2019 08:46:14|100|0.53|0.35| 62|\n",
            "|03/05/2019 08:51:14|100|0.51| 0.6| 45|\n",
            "+-------------------+---+----+----+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1c0dOzswgoq"
      },
      "source": [
        "As you can see, we have five rows, but we do not have column names. Because we did not specify a header. So Spark just created column names. Basically used a pattern `_c#`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OS_Fdl5wgoq"
      },
      "source": [
        "Spark allows us to rename the columns. By using `withColumnRenamed()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALGzAoWqwgoq"
      },
      "source": [
        "df2 = df2.withColumnRenamed('_c0','event_datetime')\\\n",
        "         .withColumnRenamed('_c1','server_id')\\\n",
        "         .withColumnRenamed('_c2','cpu_utilization')\\\n",
        "         .withColumnRenamed('_c3','free_memory')\\\n",
        "         .withColumnRenamed('_c4','session_count')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mihSpOCEwgou"
      },
      "source": [
        "Here is the new DataFrame in the tabular format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY3ZOwT4wgou",
        "outputId": "70f91f84-85e6-4891-d4e3-51b37ba3a697"
      },
      "source": [
        "df2.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---------+---------------+-----------+-------------+\n",
            "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
            "+-------------------+---------+---------------+-----------+-------------+\n",
            "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
            "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
            "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
            "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
            "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
            "|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n",
            "|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n",
            "|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n",
            "|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n",
            "|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n",
            "+-------------------+---------+---------------+-----------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBSCGwrEwgou",
        "outputId": "21e3bf3f-a742-4939-fa5b-08f775693a1b"
      },
      "source": [
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- event_datetime: string (nullable = true)\n",
            " |-- server_id: integer (nullable = true)\n",
            " |-- cpu_utilization: double (nullable = true)\n",
            " |-- free_memory: double (nullable = true)\n",
            " |-- session_count: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YwUoWhhwgov"
      },
      "source": [
        "Another useful method in DataFrame API is `describe()` that computes basic statistics for numeric and string columns.\n",
        "\n",
        "This include count, mean, stddev, min, and max."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-7_-tzHwgov",
        "outputId": "af132239-a9f6-4bc7-ef4a-4df33c033e64"
      },
      "source": [
        "df2.describe('cpu_utilization').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------------+\n",
            "|summary|    cpu_utilization|\n",
            "+-------+-------------------+\n",
            "|  count|             500000|\n",
            "|   mean| 0.6205177399999797|\n",
            "| stddev|0.15875173872913106|\n",
            "|    min|               0.22|\n",
            "|    max|                1.0|\n",
            "+-------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex4dEaWwwgov"
      },
      "source": [
        "If no columns are given, this function computes statistics for all numerical or string columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhtPPSCowgow",
        "outputId": "670bafc8-fb39-4337-e879-3968516076ff"
      },
      "source": [
        "df2.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------------+------------------+-------------------+-------------------+------------------+\n",
            "|summary|     event_datetime|         server_id|    cpu_utilization|        free_memory|     session_count|\n",
            "+-------+-------------------+------------------+-------------------+-------------------+------------------+\n",
            "|  count|             500000|            500000|             500000|             500000|            500000|\n",
            "|   mean|               null|             124.5| 0.6205177399999797|0.37912809999999125|          69.59616|\n",
            "| stddev|               null|14.430884120552715|0.15875173872913106|0.15830931278376184|14.850676696352798|\n",
            "|    min|03/05/2019 08:06:14|               100|               0.22|                0.0|                32|\n",
            "|    max|04/09/2019 01:22:46|               149|                1.0|               0.78|               105|\n",
            "+-------+-------------------+------------------+-------------------+-------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCc2TRl4wgow"
      },
      "source": [
        "### Load a JSON file into a DataFrame\n",
        "In the following cell, we are trying to load a JSON file into a DataFrame by using `spark.read` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzKWOcvVwgow"
      },
      "source": [
        "file_path = MAIN_DIRECTORY + \"/data/utilization.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKyP1uhLwgow"
      },
      "source": [
        "df3 = spark.read.format('json').load(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6T2WOESwgox"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nyWzPiRwgox"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY3xHB_Vwgox"
      },
      "source": [
        "Now, what you will notice here is we did not have to change column names.That is because in JSON, you specify key-value pairs. For example, there was a row that has `cpu_utilization` equals to 0.77, that corresponds to the first row. This row also has a key-value pair with `free_memory` equals to 0.22 and `server_id` equals to 115."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLNWxtwiwgox"
      },
      "source": [
        "Apache Spark provides an attribute called `columns`, to show the list of a DataFrame's columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIzh-c6Owgoy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QoZOrI5wgoy"
      },
      "source": [
        "Sometimes we want to work with a subset of data. For example, we have 500000 rows of data in this DataFrame. Although they are not too many rows, it may be more than you want to work with at any particular time. And you would rather work with a sample of the data. To do that, you can use `sample` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6x5gEAGwgoz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfrIBvtawgoz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRUFipGVwgoz"
      },
      "source": [
        "DataFrame API provides a method called `sort()` to sort the rows based on one or more columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2U1WxXBwgo0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSqea47Mwgo0"
      },
      "source": [
        "If we want to sort the rows based on more that one coulmn, we can specify the list of columns and sorting order by using the following syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFHZX4Eewgo0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeSQMAwWwgo1"
      },
      "source": [
        "### Filtering using DataFrame API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo_vRfI_wgo1"
      },
      "source": [
        "Now, let's take a look at how we can use DataFrame API to filter some of the rows in DataFrames.\n",
        "\n",
        "One of the DataFrames that we have created is `df1`, which stores location ID, and temperature measurement at a particular point and time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oCrIwaIwgo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a93e42f-d40c-44e5-c246-488646d5ea81"
      },
      "source": [
        "df1.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+------------+\n",
            "|         event_date|location_id|temp_celcius|\n",
            "+-------------------+-----------+------------+\n",
            "|03/04/2019 19:48:06|       loc0|          29|\n",
            "|03/04/2019 19:53:06|       loc0|          27|\n",
            "|03/04/2019 19:58:06|       loc0|          28|\n",
            "|03/04/2019 20:03:06|       loc0|          30|\n",
            "|03/04/2019 20:08:06|       loc0|          27|\n",
            "+-------------------+-----------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmNuj6-Pwgo6"
      },
      "source": [
        "If we want to filter rows based on their `location_id`, we can use `filter` command. `filter(condition)` filters rows using the given condition. `filter()` method essentially allows us to specify a `WHERE` clause."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzWA-7zwwgo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5610dd3b-b07f-49d9-ac6f-5cbd21206a57"
      },
      "source": [
        "df1.filter(df1['location_id']=='loc10').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+------------+\n",
            "|         event_date|location_id|temp_celcius|\n",
            "+-------------------+-----------+------------+\n",
            "|03/04/2019 19:48:08|      loc10|          26|\n",
            "|03/04/2019 19:53:08|      loc10|          31|\n",
            "|03/04/2019 19:58:08|      loc10|          25|\n",
            "|03/04/2019 20:03:08|      loc10|          25|\n",
            "|03/04/2019 20:08:08|      loc10|          26|\n",
            "|03/04/2019 20:13:08|      loc10|          23|\n",
            "|03/04/2019 20:18:08|      loc10|          26|\n",
            "|03/04/2019 20:23:08|      loc10|          23|\n",
            "|03/04/2019 20:28:08|      loc10|          31|\n",
            "|03/04/2019 20:33:08|      loc10|          23|\n",
            "|03/04/2019 20:38:08|      loc10|          27|\n",
            "|03/04/2019 20:43:08|      loc10|          23|\n",
            "|03/04/2019 20:48:08|      loc10|          28|\n",
            "|03/04/2019 20:53:08|      loc10|          26|\n",
            "|03/04/2019 20:58:08|      loc10|          26|\n",
            "|03/04/2019 21:03:08|      loc10|          27|\n",
            "|03/04/2019 21:08:08|      loc10|          23|\n",
            "|03/04/2019 21:13:08|      loc10|          25|\n",
            "|03/04/2019 21:18:08|      loc10|          25|\n",
            "|03/04/2019 21:23:08|      loc10|          26|\n",
            "+-------------------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WntVbbFQwgo7"
      },
      "source": [
        "If we want to count all the rows that are located in a specific `location_id`,we can specify the `count()` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNqoceuswgo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618bfef7-aaf7-495b-ee53-2fbef272c177"
      },
      "source": [
        "df1.filter(df1['location_id']=='loc10').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opbPBxZGwgo7"
      },
      "source": [
        "Sometimes we only need to list one or two columns; in this case, we can use `select()` method that projects a set of expressions and returns a new DataFrame. Let's take a look at how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CP3Up_zwgo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0915b51-4d8d-479f-a672-f36767e731d2"
      },
      "source": [
        "df1.select(['location_id','temp_celcius']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+------------+\n",
            "|location_id|temp_celcius|\n",
            "+-----------+------------+\n",
            "|       loc0|          29|\n",
            "|       loc0|          27|\n",
            "|       loc0|          28|\n",
            "|       loc0|          30|\n",
            "|       loc0|          27|\n",
            "|       loc0|          27|\n",
            "|       loc0|          27|\n",
            "|       loc0|          29|\n",
            "|       loc0|          32|\n",
            "|       loc0|          35|\n",
            "|       loc0|          32|\n",
            "|       loc0|          28|\n",
            "|       loc0|          28|\n",
            "|       loc0|          32|\n",
            "|       loc0|          34|\n",
            "|       loc0|          33|\n",
            "|       loc0|          27|\n",
            "|       loc0|          28|\n",
            "|       loc0|          33|\n",
            "|       loc0|          28|\n",
            "+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB0gYKl5wgo8"
      },
      "source": [
        "### Aggregation using DataFrame API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkiIkDeGwgo8"
      },
      "source": [
        "Now, let's take a look at aggregating using the DataFrame API. In the following cell we will use `groupBy` method that groups the DataFrame using the specified columns, so we can run aggregation on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0DG1BhQwgo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75de79db-9e55-4b2e-b2fa-1cb4e89210f1"
      },
      "source": [
        "df1.groupBy('location_id').max().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|location_id|\n",
            "+-----------+\n",
            "|     loc196|\n",
            "|     loc226|\n",
            "|     loc463|\n",
            "|     loc150|\n",
            "|     loc292|\n",
            "|     loc311|\n",
            "|      loc22|\n",
            "|     loc351|\n",
            "|     loc370|\n",
            "|     loc419|\n",
            "|      loc31|\n",
            "|     loc305|\n",
            "|      loc82|\n",
            "|      loc90|\n",
            "|     loc118|\n",
            "|     loc195|\n",
            "|     loc208|\n",
            "|      loc39|\n",
            "|      loc75|\n",
            "|     loc228|\n",
            "+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUD1E7Aywgo9"
      },
      "source": [
        "If we want to sort the DataFrame, we can use `orderBy` that returns a new DataFrame sorted by the specified column(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5Yw9Ybwgo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a66c095-1ec3-4f5e-ecae-a3c4cf1dc4e1"
      },
      "source": [
        "df1.orderBy('location_id', ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+------------+\n",
            "|         event_date|location_id|temp_celcius|\n",
            "+-------------------+-----------+------------+\n",
            "|03/04/2019 21:18:25|      loc99|          32|\n",
            "|03/04/2019 20:38:25|      loc99|          34|\n",
            "|03/04/2019 21:13:25|      loc99|          37|\n",
            "|03/04/2019 20:13:25|      loc99|          37|\n",
            "|03/04/2019 20:33:25|      loc99|          32|\n",
            "|03/04/2019 20:53:25|      loc99|          33|\n",
            "|03/04/2019 21:08:25|      loc99|          31|\n",
            "|03/04/2019 19:53:25|      loc99|          34|\n",
            "|03/04/2019 20:08:25|      loc99|          31|\n",
            "|03/04/2019 20:23:25|      loc99|          31|\n",
            "|03/04/2019 20:28:25|      loc99|          34|\n",
            "|03/04/2019 20:43:25|      loc99|          36|\n",
            "|03/04/2019 20:48:25|      loc99|          34|\n",
            "|03/04/2019 20:58:25|      loc99|          32|\n",
            "|03/04/2019 21:03:25|      loc99|          32|\n",
            "|03/04/2019 19:48:25|      loc99|          33|\n",
            "|03/04/2019 21:28:25|      loc99|          33|\n",
            "|03/04/2019 19:58:25|      loc99|          32|\n",
            "|03/04/2019 20:03:25|      loc99|          32|\n",
            "|03/04/2019 20:18:25|      loc99|          35|\n",
            "+-------------------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7X8hdEcwgo-"
      },
      "source": [
        "To calculate the average temperature at each location, we can use `agg` operation. Let's take a look at the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IPIo-Cdwgo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca8ade3-c571-4f66-ad1b-a948cda1fd79"
      },
      "source": [
        "df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------------+\n",
            "|location_id|avg(temp_celcius)|\n",
            "+-----------+-----------------+\n",
            "|       loc0|           29.176|\n",
            "|       loc1|           28.246|\n",
            "|      loc10|           25.337|\n",
            "|     loc100|           27.297|\n",
            "|     loc101|           25.317|\n",
            "|     loc102|           30.327|\n",
            "|     loc103|           25.341|\n",
            "|     loc104|           26.204|\n",
            "|     loc105|           26.217|\n",
            "|     loc106|           27.201|\n",
            "|     loc107|           33.268|\n",
            "|     loc108|           32.195|\n",
            "|     loc109|           24.138|\n",
            "|      loc11|           25.308|\n",
            "|     loc110|           26.239|\n",
            "|     loc111|           31.391|\n",
            "|     loc112|           33.359|\n",
            "|     loc113|           30.345|\n",
            "|     loc114|           29.261|\n",
            "|     loc115|           23.239|\n",
            "+-----------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoArQMxFwgo-"
      },
      "source": [
        "There are different aggregation function options, for example, if we want to have the maximum temperature in each location, we can write the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQZ7aa-ywgo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beff6559-3d95-4969-9895-ac649a71a0cc"
      },
      "source": [
        "df1.groupBy('location_id').agg({'temp_celcius':'max'}).orderBy('location_id', ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------------+\n",
            "|location_id|max(temp_celcius)|\n",
            "+-----------+-----------------+\n",
            "|      loc99|               40|\n",
            "|      loc98|               39|\n",
            "|      loc97|               38|\n",
            "|      loc96|               35|\n",
            "|      loc95|               40|\n",
            "|      loc94|               32|\n",
            "|      loc93|               31|\n",
            "|      loc92|               40|\n",
            "|      loc91|               37|\n",
            "|      loc90|               30|\n",
            "|       loc9|               39|\n",
            "|      loc89|               37|\n",
            "|      loc88|               32|\n",
            "|      loc87|               38|\n",
            "|      loc86|               30|\n",
            "|      loc85|               35|\n",
            "|      loc84|               33|\n",
            "|      loc83|               33|\n",
            "|      loc82|               34|\n",
            "|      loc81|               30|\n",
            "+-----------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TZPh4lfwgo_"
      },
      "source": [
        "### Data Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daUbrgdmwgo_"
      },
      "source": [
        "Sometimes, we may want to use sampling, particularly when we have large data sets, and we are doing kind of an exploratory analysis. We want to get kind of an understanding at a high level of what the data is like. Sampling can be beneficial for doing quick operations. Now, let's see how we can take a sample, or a subset of that, but randomly. In PySpark, `sample()` method returns a sampled subset of this DataFrame, and it usually takes two parameters, `fraction` that specifies the fraction of rows to generate, range [0.0, 1.0]. The second parameter is `withReplacement`, which is a boolean parameter. Usually, we assign `false` to it, in this case, what that means is each time we pull a row out of our sampling, we don't put it back in, so we will never get duplicates, we will always get unique values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNUWlCGYwgo_"
      },
      "source": [
        "df1_sample = df1.sample(fraction=0.1,withReplacement=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH76pgYywgpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138ecc16-80aa-406c-9dd4-5e3be7dd37ba"
      },
      "source": [
        "df1_sample.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50060"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0FF7f-VwgpA"
      },
      "source": [
        "Now, let's run some simple descriptive statistics on our sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h5tc-PXwgpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e3bfdc-dab5-4454-a2bd-a5bb97b6b71c"
      },
      "source": [
        "df1_sample.groupBy('location_id').agg({'temp_celcius':'mean'}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+------------------+\n",
            "|location_id| avg(temp_celcius)|\n",
            "+-----------+------------------+\n",
            "|     loc196|28.982456140350877|\n",
            "|     loc226|25.147058823529413|\n",
            "|     loc463|22.989247311827956|\n",
            "|     loc150| 32.12173913043478|\n",
            "|     loc292| 29.40740740740741|\n",
            "|     loc311|24.192660550458715|\n",
            "|      loc22| 28.20754716981132|\n",
            "|     loc351|27.801980198019802|\n",
            "|     loc370|29.216494845360824|\n",
            "|     loc419|29.079646017699115|\n",
            "|      loc31|25.236842105263158|\n",
            "|     loc305| 27.38095238095238|\n",
            "|      loc82|27.391752577319586|\n",
            "|      loc90| 22.94736842105263|\n",
            "|     loc118| 24.55855855855856|\n",
            "|     loc195|27.448275862068964|\n",
            "|     loc208|26.377551020408163|\n",
            "|      loc39| 25.44047619047619|\n",
            "|      loc75|22.871287128712872|\n",
            "|     loc228| 27.09090909090909|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hv1JhKwgpA"
      },
      "source": [
        "Now, let's compare these results to results of the original data set, the DataFrame `df1`, which has 500000 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ToVm1MdwgpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3fdc36-432d-46fe-c606-7afcaabd1cc2"
      },
      "source": [
        "df1.groupBy('location_id').agg({'temp_celcius':'mean'}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------------+\n",
            "|location_id|avg(temp_celcius)|\n",
            "+-----------+-----------------+\n",
            "|     loc196|           29.225|\n",
            "|     loc226|           25.306|\n",
            "|     loc463|           23.317|\n",
            "|     loc150|           32.188|\n",
            "|     loc292|           29.159|\n",
            "|     loc311|           24.308|\n",
            "|      loc22|           28.251|\n",
            "|     loc351|           28.194|\n",
            "|     loc370|            29.14|\n",
            "|     loc419|           29.141|\n",
            "|      loc31|           25.196|\n",
            "|     loc305|           27.314|\n",
            "|      loc82|           27.355|\n",
            "|      loc90|           23.216|\n",
            "|     loc118|           24.219|\n",
            "|     loc195|            27.25|\n",
            "|     loc208|           26.206|\n",
            "|      loc39|           25.199|\n",
            "|      loc75|           23.209|\n",
            "|     loc228|           27.295|\n",
            "+-----------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrNTLyLYwgpB"
      },
      "source": [
        "As you can see, when we did the sampling and took 10% when we took the average of location zero, we got something that was about 29.4, but the actual is approximately 29.18. Therefore, we can see by sampling, we get very close to what the average is for the actual population. One of the things to consider is the size of the sample that we are drawing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8_4IUn0wgpB"
      },
      "source": [
        "### Save Data from DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOba1tipwgpB"
      },
      "source": [
        "Sometimes after we have been working with DataFrames and creating new DataFrames and running calculations and doing sampling, we might want to save our results out. To do this, we can use `write` object and specify the `csv()` method within that, and then specify a name or what we'd like to save. It saves the DataFrame to disk using the csv format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqDHIVtwgpC"
      },
      "source": [
        "df1.write.csv('df1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WCP7qSpwgpC"
      },
      "source": [
        "Now, let's take a look at the current directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLUo8JAZwgpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8642de83-503d-4478-aea5-1c07ac35fae3"
      },
      "source": [
        "! Is df1.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: Is: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NpWamlawgpC"
      },
      "source": [
        "Now, what you will notice here is that `df1.csv` is not a single file. It is a directory. And what is in that directory is four different files with `csv` extensions, and that is because of the way Apache Spark works internally. Spark can break up DataFrames into partition subsets, and in this case, there were four partitions. Each partition has its own file. There is also a `_SUCCESS` flag that was written out. Now, let's list the contents of one of these files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GaSbR2ZwgpD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QANFCM4wgpF"
      },
      "source": [
        "To write the DataFrame in JSON format, you can use the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omkLIkgJwgpF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxzvxgfvwgpG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvLjO-drwgpG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8-sZgW9wgpG"
      },
      "source": [
        "## Querying DataFrames with SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJB8xgMnwgpH"
      },
      "source": [
        "Up to now, we've been using the Spark DataFrame API to work with DataFrames. Now, we're going to switch gears and we're going to work with SQL. In particular, we're going to use Spark SQL for working with DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPenR9oswgpH"
      },
      "source": [
        "In this part, we will use `utilization.json` that includes cpu utilization, the amount of free memory at a particular point in time, and then the number of sessions that are currently connected to the server at the particular point in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO0Whgb6wgpH"
      },
      "source": [
        "file_path = MAIN_DIRECTORY + '/data/utilization.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVE2nrwOwgpI"
      },
      "source": [
        "df_util = spark.read.format('json').load(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6BWt_aewgpI"
      },
      "source": [
        "df_util.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy9Wzp_Vf-A_"
      },
      "source": [
        "df_util.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP4ZETTUgCu1"
      },
      "source": [
        "df_util.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csZ8WYETwgpI"
      },
      "source": [
        "To work with SQL in Spark, we have to create a temporary view. And to do that, we specify the DataFrame, and then we call the method `createOrReplaceTempView()` and then we should specify a name for this table. Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kgLDfMwgpJ"
      },
      "source": [
        "df_util.createOrReplaceTempView('utilization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1aLbg4iwgpJ"
      },
      "source": [
        "Now, we have the ability to query on a table called utilization. We will create that by executing a SQL command in the Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBBgnlCuwgpJ"
      },
      "source": [
        "df_sql = spark.sql(\"SELECT * FROM utilization LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XignA3BzwgpJ"
      },
      "source": [
        "df_sql.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzT0mQKxwgpK"
      },
      "source": [
        "If we want to project on specific columns, we can do it in the following way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3_-aShNwgpK"
      },
      "source": [
        "spark.sql(\"SELECT server_id, free_memory FROM utilization LIMIT 10\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI4H_G6awgpK"
      },
      "source": [
        "### Filtering DataFrames with SQL\n",
        "Next, we are going to take a look at how to filter DataFrames using Spark SQL.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiiAInXjwgpK"
      },
      "source": [
        "spark.sql(\"SELECT * FROM utilization WHERE server_id = 149 LIMIT 10\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgg08YRKj39y"
      },
      "source": [
        "spark.sql(\"SELECT server_id as SID, session_count as sc \\\n",
        "          FROM utilization WHERE session_count > 50 \\\n",
        "          LIMIT 10\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GfFfQWJwgpL"
      },
      "source": [
        "spark.sql(\"SELECT server_id, session_count FROM utilization \\\n",
        "          WHERE session_count > 70 AND server_id = 120 \\\n",
        "          ORDER BY session_count DESC \\\n",
        "          LIMIT 10\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUa_UUexwgpL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqMwL6z6wgpL"
      },
      "source": [
        "### Aggregation DataFrames with SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44v-bzh0wgpL"
      },
      "source": [
        "When we work with SQL in databases, we often use SQL to perform aggregations and the same holds true when working with SQL in Spark. Let's write some basic queries against the DataFrame and do a very simple aggregations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wKnLNl9wgpM"
      },
      "source": [
        "spark.sql(\"SELECT count(*) FROM utilization\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k1fC_GPwgpM"
      },
      "source": [
        "spark.sql(\"SELECT count(*) FROM utilization WHERE session_count > 70\").show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzKXuBpDnvPl"
      },
      "source": [
        "rk.sql(\"SELECT server_id, count(*) FROM utilization \\\n",
        "       WHERE session_count > 70 \\\n",
        "       GROUP BY server_id \\\n",
        "       ORDER BY count(*) DESC\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBPWu7q1wgpM"
      },
      "source": [
        "spark.sql(\"SELECT server_id, min(session_count) as Min, max(session_count) as Max, \\\n",
        "          round(avg(session_count),2) as Average\\\n",
        "          FROM utilization \\\n",
        "          WHERE session_count > 70 \\\n",
        "          GROUP BY server_id \\\n",
        "          ORDER BY count(*) DESC\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfk7czTbwgpM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYPFFXFWwgpN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-RTITA4wgpN"
      },
      "source": [
        "### Joining DataFrames with SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgblkmDgwgpN"
      },
      "source": [
        "One of the most useful features of SQL is the ability to join tables. We can join in Spark SQL as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj5I3BzfwgpN"
      },
      "source": [
        "First, we are going to create another temporary table based on the `server_names.csv` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRbESzsbwgpN"
      },
      "source": [
        "file_path = MAIN_DIRECTORY + '/data/server_names.csv'\n",
        "df_servers = spark.read.csv(file_path, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clz-NgYZwgpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5965b2-a5c1-4467-c9c3-7bfc75bc0904"
      },
      "source": [
        "df_servers.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----------+\n",
            "|server_id|server_name|\n",
            "+---------+-----------+\n",
            "|      100| Server 100|\n",
            "|      101| Server 101|\n",
            "|      102| Server 102|\n",
            "|      103| Server 103|\n",
            "|      104| Server 104|\n",
            "|      105| Server 105|\n",
            "|      106| Server 106|\n",
            "|      107| Server 107|\n",
            "|      108| Server 108|\n",
            "|      109| Server 109|\n",
            "|      110| Server 110|\n",
            "|      111| Server 111|\n",
            "|      112| Server 112|\n",
            "|      113| Server 113|\n",
            "|      114| Server 114|\n",
            "|      115| Server 115|\n",
            "|      116| Server 116|\n",
            "|      117| Server 117|\n",
            "|      118| Server 118|\n",
            "|      119| Server 119|\n",
            "+---------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBpVFlwwgpO"
      },
      "source": [
        "df_servers.createOrReplaceGlobalTempView('server_names')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQSTyr0wgpO"
      },
      "source": [
        "Now, let's quickly do a check on `server_id` in `utilization` table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LorUr5WwgpP"
      },
      "source": [
        "spark.sql(\"SELECT DISTINCT server_id FROM utilization ORDER BY server_id\").show(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hQmUb07wgpP"
      },
      "source": [
        "Now, let's see what the minimum and maximum of server_id is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFr_1aylwgpP"
      },
      "source": [
        "spark.sql(\"SELECT min(server_id), max(server_id) FROM server_names\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpuCjZ9wgpP"
      },
      "source": [
        "Well, let's join these two tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otZnCIOjwgpQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBw_qei6wgpQ"
      },
      "source": [
        "### De-Duplicating with DataFrame API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t597SnBwgpQ"
      },
      "source": [
        "When we're working with Data Frames, Spark provides some ways to de-duplicate data. So, let's take a look at how to do that. In this part also we will learn how we can create small data sets to work within the Jupiter Notebook session. Before doing anything, please restart the Jupyter kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykzHN2uMwgpQ"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import Row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uytJbYKVwgpR"
      },
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI9covdiwgpR"
      },
      "source": [
        "`sc` stands for `SparkContext`. It is a global variable that gives us access to the Spark Context. Here, what we want to do is create a DataFrame, and to do that, we will use `parallelize` method that creates a parallelized data structure. Spark automatically parallelize DataFrames. But now we are going to create this data manually, so we are specifying `parallelized` explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1R-hl7GwgpR"
      },
      "source": [
        "rdd = sc.parallelize([Row(server_name = 'Server 101', cpu_utilization=85, session_count = 80),\n",
        "                      Row(Server_name = 'Server 101', cpu_utilization=80, session_count = 90),\n",
        "                      Row(Server_name = 'Server 102', cpu_utilization=85, session_count = 80),\n",
        "                      Row(Server_name = 'Server 102', cpu_utilization=85, session_count = 80)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCsNNU_IwgpR"
      },
      "source": [
        "spark_1 = SparkSession(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsT-MbXIwgpS"
      },
      "source": [
        "`toDF()` turns that parallelized data structure to into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcwbzYnvwgpS"
      },
      "source": [
        "df_dup = rdd.toDF()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5sH196gzQu1",
        "outputId": "4cfa267c-f54a-467c-c050-9478e80c5321"
      },
      "source": [
        "df_dup.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+\n",
            "|server_name|cpu_utilization|session_count|\n",
            "+-----------+---------------+-------------+\n",
            "| Server 101|             85|           80|\n",
            "| Server 101|             80|           90|\n",
            "| Server 102|             85|           80|\n",
            "| Server 102|             85|           80|\n",
            "+-----------+---------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9uCDbD6wgpT"
      },
      "source": [
        "Now, we are going to drop duplicates. To do that we can use `drop_duplicates()` method which returns a new DataFrame with duplicate rows removed, optionally only considering certain columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58pR_75JwgpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111a20d2-a28b-4350-c7fa-083383ed4f62"
      },
      "source": [
        " df_dup.drop_duplicates().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+\n",
            "|server_name|cpu_utilization|session_count|\n",
            "+-----------+---------------+-------------+\n",
            "| Server 101|             85|           80|\n",
            "| Server 102|             85|           80|\n",
            "| Server 101|             80|           90|\n",
            "+-----------+---------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bBHt3CxwgpT"
      },
      "source": [
        "If we want to drop any time there is a duplicate in one of the columns, we can do that as well. Let's take a look at the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR_j4k5RwgpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5eb25ce-77e2-402c-9444-eb4768a0697a"
      },
      "source": [
        " df_dup.drop_duplicates(['server_name']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+\n",
            "|server_name|cpu_utilization|session_count|\n",
            "+-----------+---------------+-------------+\n",
            "| Server 101|             85|           80|\n",
            "| Server 102|             85|           80|\n",
            "+-----------+---------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIS48urwwgpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e432a5-81d2-4524-a90a-9db8b07553b4"
      },
      "source": [
        "df_dup.drop_duplicates(['server_name','cpu_utilization']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+\n",
            "|server_name|cpu_utilization|session_count|\n",
            "+-----------+---------------+-------------+\n",
            "| Server 101|             80|           90|\n",
            "| Server 101|             85|           80|\n",
            "| Server 102|             85|           80|\n",
            "+-----------+---------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgTbMjgAwgpU"
      },
      "source": [
        "### Working with null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlk7VnKwgpU"
      },
      "source": [
        "It is not uncommon to have data missing from DataFrame. When we are working with SQL, we are used to work with nulls. When we working with DataFrames, the absence of data is indicated by an NA. So in this section, we are going to look how we can work with NAs and Nulls using DataFrames and Spark SQL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STzNBpHmwgpV"
      },
      "source": [
        "In this section, we are importing a couple of things, we have not seen before. Let's take a look at them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwzgCnw2wgpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9933de-27a5-4697-a2ad-85871161a4c1"
      },
      "source": [
        "df_dup.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+\n",
            "|server_name|cpu_utilization|session_count|\n",
            "+-----------+---------------+-------------+\n",
            "| Server 101|             85|           80|\n",
            "| Server 101|             80|           90|\n",
            "| Server 102|             85|           80|\n",
            "| Server 102|             85|           80|\n",
            "+-----------+---------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE8uMhXZwgpV"
      },
      "source": [
        "Now, we are going to add a column and set that column's values equall to null or NA. In this case, we will use `lit()` function that is a way for us to interact with column literals in PySpark. Spark SQL functions lit() is used to add a new column by assigning a literal or constant value to Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBnN71h9wgpV"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import StringType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd82EqajwgpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a35dd49-6ca8-4757-8a2c-32b8faf1da45"
      },
      "source": [
        "df_na = df_dup.withColumn('na_col',lit(None).cast(StringType()))\n",
        "df_na.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+------+\n",
            "|server_name|cpu_utilization|session_count|na_col|\n",
            "+-----------+---------------+-------------+------+\n",
            "| Server 101|             85|           80|  null|\n",
            "| Server 101|             80|           90|  null|\n",
            "| Server 102|             85|           80|  null|\n",
            "| Server 102|             85|           80|  null|\n",
            "+-----------+---------------+-------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqgcX6ivwgpW"
      },
      "source": [
        "Now, one of the things that we can do is globally replace all nulls or NAs with some value. And we can do that with `fillna()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Ib-pQAwgpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42a44da-9f3f-42a4-c7a0-a92020768f93"
      },
      "source": [
        "df_na.fillna('A').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+------+\n",
            "|server_name|cpu_utilization|session_count|na_col|\n",
            "+-----------+---------------+-------------+------+\n",
            "| Server 101|             85|           80|     A|\n",
            "| Server 101|             80|           90|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "+-----------+---------------+-------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxRH8w9hwgpW"
      },
      "source": [
        "Now, Let's create a DataFrame that has versions both with the nulls and with the As."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3HAaXiXwgpX"
      },
      "source": [
        "df_union = df_na.fillna('A').union(df_na)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keHWOxtuwgpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153cd9c8-413f-470b-e588-8c8038f31a4e"
      },
      "source": [
        "df_union.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+------+\n",
            "|server_name|cpu_utilization|session_count|na_col|\n",
            "+-----------+---------------+-------------+------+\n",
            "| Server 101|             85|           80|     A|\n",
            "| Server 101|             80|           90|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "| Server 101|             85|           80|  null|\n",
            "| Server 101|             80|           90|  null|\n",
            "| Server 102|             85|           80|  null|\n",
            "| Server 102|             85|           80|  null|\n",
            "+-----------+---------------+-------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuHcQgEOwgpX"
      },
      "source": [
        "Now we can drop only rows with NAs in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZkkYerTwgpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71801da-24bf-47b2-8987-fb5b4e0fc870"
      },
      "source": [
        "df_union.na.drop().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+------+\n",
            "|server_name|cpu_utilization|session_count|na_col|\n",
            "+-----------+---------------+-------------+------+\n",
            "| Server 101|             85|           80|     A|\n",
            "| Server 101|             80|           90|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "+-----------+---------------+-------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBX3NvIOwgpY"
      },
      "source": [
        "Well, let's see how we can do that with Spark SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPLs6W_iwgpY"
      },
      "source": [
        "df_union.createOrReplaceTempView('na_table')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll0rjebiwgpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ebaed1-427d-472b-a684-66d7e74fa19b"
      },
      "source": [
        "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NOT NULL\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+------+\n",
            "|server_name|cpu_utilization|session_count|na_col|\n",
            "+-----------+---------------+-------------+------+\n",
            "| Server 101|             85|           80|     A|\n",
            "| Server 101|             80|           90|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "| Server 102|             85|           80|     A|\n",
            "+-----------+---------------+-------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrcDNVzN13Rv",
        "outputId": "7ef53dd6-f2b0-456f-aaaa-7019881f6638"
      },
      "source": [
        "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NULL\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------+-------------+------+\n",
            "|server_name|cpu_utilization|session_count|na_col|\n",
            "+-----------+---------------+-------------+------+\n",
            "| Server 101|             85|           80|  null|\n",
            "| Server 101|             80|           90|  null|\n",
            "| Server 102|             85|           80|  null|\n",
            "| Server 102|             85|           80|  null|\n",
            "+-----------+---------------+-------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKR7y9vHwgpY"
      },
      "source": [
        "## Exploratory Data Analysis with DataFrame API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTBtMALwgpZ"
      },
      "source": [
        "DataFrame API provides some tools for some higher level tasks like exploratory data analysis. In this section, we are going to learn how to use DataFrame API for doing some basic EDA with the utilization DataFrame. First, let's take a look at this DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDFiiYcywgpZ"
      },
      "source": [
        "df_util.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnxtPidawgpZ"
      },
      "source": [
        "df_util.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_d8ZDmwgpZ"
      },
      "source": [
        "One of the useful methods for doing exploratory data analysis is `.describe()`. Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sjzp6mUwgpa"
      },
      "source": [
        "df_util.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT23bnxHwgpa"
      },
      "source": [
        "`.describe()` actually produces another DataFrame with summary statistics about the DataFrame. For example, in this case, we see that there are several columns; there is a summary column, followed by the name of a column in the original DataFrame. For each of those columns in the original DataFrame, we have the same statistics that are calculated.\n",
        "Using `.describe()`  is an excellent way to get a high-level view of what a data set might be like.\n",
        "\n",
        "Another statistics we often want to know, is there a correlation between two of the variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5QGC4lnwgpa"
      },
      "source": [
        "df_util.stat.corr('cpu_utilization','free_memory')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu8yl1fkwgpb"
      },
      "source": [
        "df_util.stat.corr('session_count','free_memory')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuxuZ0Y1wgpb"
      },
      "source": [
        "Sometimes we want to know how frequent are some items, what are the most frequently occurring items?\n",
        "\n",
        "There is a method called `.freq()` items for frequent items, which we can use with the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJvoMqvewgpb"
      },
      "source": [
        "df_util.stat.fregItem(['server_id']).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONTwUOWwgpb"
      },
      "source": [
        "We can create a result-set that shows some basic statistics for one of the columns by using Spark SQL. Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc1vJIXiwgpc"
      },
      "source": [
        "spark.sql(\"SELECT min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization), avg(cpu_utilization) FROM utilization\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af3GSD6Mwgpc"
      },
      "source": [
        "And if we want to group the result-set by `server_id`, we can write the following query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URsmehZwgpd"
      },
      "source": [
        "spark.sql(\"SELECT min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization), avg(cpu_utilization) FROM utilization\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtm7Sj4fwgpd"
      },
      "source": [
        "Now, we are going to calculate statistics on buckets or histograms of data. The idea is, rather than look at each server individually, Spark buckets values according to how frequently they occur in certain ranges. So if we want to know how often does a CPU utilization fall in the range of 1-10 or 11-20 or 21-30, all the way up to 90-91, we could put each of those CPU utilization measures into its bucket and count how many times a CPU utilization goes into that bucket. Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-EfLr75wgpd"
      },
      "source": [
        "spark.sql(\"SELECT count(*), FLOOR(cpu_utilization * 100 / 10) as Bucket \\\n",
        "FROM utilization \\\n",
        "GROUP BY Bucket \\\n",
        "ORDER BY Bucket\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T41mMLBKwgpe"
      },
      "source": [
        "So far, what we have done is we have listed for each server in what  CPU utilization bucket falls at a particular time. Now we want to see how often does a CPU utilization falls into one of those ten buckets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ggE3nkwgpe"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQEl4pQfwgpe"
      },
      "source": [
        "## Timeseries Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrFvGronwgpf"
      },
      "source": [
        "In this section, we are going to work with timeseries data, and timeseries data has a set of measures and a timestamp associated with them. First, let's take a look at utilization table again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_ARlHBNwgpf"
      },
      "source": [
        "spark.sql(\"SELECT server_id, min(cpu_utilization), max(cpu_utilization), avg(cpu_utilization),stddev(cpu_utilization) \\\n",
        "          FROM utilization \\\n",
        "          GROUP BY server_id\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rixki5gBwgpf"
      },
      "source": [
        "Sometimes we might want to compare a value within a group. For example, we would like to compare the current CPU utilization for a server to the average CPU utilization of just that server, not the entire population.\n",
        "\n",
        "We can do that using a windowing function, and in SQL, the windowing functions are specified using an `OVER...PARTITION BY` statement. So let's take a look at how to use that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NWS4-8Kwgpf"
      },
      "source": [
        "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
        "          avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util \\\n",
        "          FROM utilization\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8reHd7zwgpg"
      },
      "source": [
        "Now, we have different timestamps for each server ID, different CPU utilization at those particular times, but in this piece of result-set, the average server utilization is always 0.7153 for server ID 112.\n",
        "\n",
        "Now, we want to calculate the difference any one of these measurements of CPU utilization from the average of that server is?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDMrw4iSwgpg"
      },
      "source": [
        "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
        "          avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util,\\\n",
        "          cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) as delta\\\n",
        "          FROM utilization\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWyoJmi4wgpg"
      },
      "source": [
        "That is one of the operations that we can do with the windowing functions. We can compare a particular value in a row to a value of some aggregate function applied to a sub-set of rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6klDV-Phwgpg"
      },
      "source": [
        "Another operation that we can do with windowing functions is looking around the neighborhood of a row. For example, we might want to calculate in a sliding window, look at the last three values and average them or look at the previous value, current value, next value, and average them. Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdISAkKwgph"
      },
      "source": [
        "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
        "          avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n",
        "          ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as avg_cpu_util \\\n",
        "          FROM utilization\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7CSwYEvwgph"
      },
      "source": [
        "#### Great Job!"
      ]
    }
  ]
}