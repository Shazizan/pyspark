{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shazizan/pyspark/blob/main/ApacheSpark_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y85pMOEC1VH"
      },
      "source": [
        "<img src=\"images/cads-logo.png\" style=\"height: 100px;padding-top:5px\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%; padding-top:0px\" align=right>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hATRxl-YC1VS"
      },
      "source": [
        "# Manipulating Data using Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pVIt1pTC1VW"
      },
      "source": [
        "In this notebook, we are going to get our hands dirty with Spark DataFrame API to perform common data operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usCuHKF5C1VY",
        "outputId": "ed0c312c-33b1-44a1-fd0a-7b49df60fa63"
      },
      "source": [
        "! pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 27.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=59006a5e083ff1221cdd57bf93b6d855416456101df07cc4926e5deed0b9ed8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrOejgCJC1Va"
      },
      "source": [
        "If you take a look at dataset folder, you will see `flights.csv` that contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015.\n",
        "\n",
        "In the first step, we should create a DataFrame using `flights.csv` file and then create a table (temporaray view) for querying flights by using SQL commands. Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWqQScniC1Vg"
      },
      "source": [
        "import os\n",
        "MAIN_DIRECTORY = os.getcwd()\n",
        "file_path =MAIN_DIRECTORY+\"/dataset/flights.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6tiSXeiC1Vi"
      },
      "source": [
        "df_flights = spark.read.format(\"csv\").option(\"header\",\"true\").option('inferSchema','true').load(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEKDE11kC1V5"
      },
      "source": [
        "# A simple way to create a dataframe in Spark\n",
        "# df_flights = spark.read.csv(file_path, header=True, inferSchema = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLRwwZcEC1V-"
      },
      "source": [
        "df_flights.createOrReplaceTempView('flights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z_lwZf0C1WB"
      },
      "source": [
        "### Exercise 1: Use SQL to get the first five rows of the flights table and save the result to flights5, finally show the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5GWLlfm1z6J",
        "outputId": "1016052e-0969-4f25-d145-ebcece3069b2"
      },
      "source": [
        "flights5 = spark.sql(\"SELECT * FROM flights LIMIT 5\")\n",
        "flights5.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
            "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
            "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
            "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
            "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbxG8IeqC1WE"
      },
      "source": [
        "### Exercise 2: Write a query that counts the number of flights to each airport from SEA and PDX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjX4EIL3C1WG",
        "outputId": "0e03d99f-9f2e-4cf1-ec5b-0b43c75c4893"
      },
      "source": [
        "spark.sql(\"SELECT origin, dest, count(*) FROM flights GROUP BY origin , dest\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+--------+\n",
            "|origin|dest|count(1)|\n",
            "+------+----+--------+\n",
            "|   SEA| RNO|       8|\n",
            "|   SEA| DTW|      98|\n",
            "|   SEA| CLE|       2|\n",
            "|   SEA| LAX|     450|\n",
            "|   PDX| SEA|     144|\n",
            "|   SEA| BLI|       5|\n",
            "|   PDX| IAH|      57|\n",
            "|   PDX| PHX|     209|\n",
            "|   SEA| SLC|     225|\n",
            "|   SEA| SBA|      23|\n",
            "|   SEA| BWI|      29|\n",
            "|   PDX| IAD|      23|\n",
            "|   PDX| SFO|     305|\n",
            "|   SEA| KOA|      40|\n",
            "|   PDX| MCI|      15|\n",
            "|   SEA| SJC|     213|\n",
            "|   SEA| ABQ|      43|\n",
            "|   SEA| SAT|      18|\n",
            "|   PDX| ONT|      57|\n",
            "|   SEA| LAS|     364|\n",
            "+------+----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9flDyw32e8s",
        "outputId": "9208515b-e551-465a-9b1b-15c9c6d8b9cd"
      },
      "source": [
        "df_flights.groupBy(['origin','dest']).count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+\n",
            "|origin|dest|count|\n",
            "+------+----+-----+\n",
            "|   SEA| RNO|    8|\n",
            "|   SEA| DTW|   98|\n",
            "|   SEA| CLE|    2|\n",
            "|   SEA| LAX|  450|\n",
            "|   PDX| SEA|  144|\n",
            "|   SEA| BLI|    5|\n",
            "|   PDX| IAH|   57|\n",
            "|   PDX| PHX|  209|\n",
            "|   SEA| SLC|  225|\n",
            "|   SEA| SBA|   23|\n",
            "|   SEA| BWI|   29|\n",
            "|   PDX| IAD|   23|\n",
            "|   PDX| SFO|  305|\n",
            "|   SEA| KOA|   40|\n",
            "|   PDX| MCI|   15|\n",
            "|   SEA| SJC|  213|\n",
            "|   SEA| ABQ|   43|\n",
            "|   SEA| SAT|   18|\n",
            "|   PDX| ONT|   57|\n",
            "|   SEA| LAS|  364|\n",
            "+------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rilRVV0C1WH"
      },
      "source": [
        "### Exercise 3: Write a piece of code to create a DataFrame using `airports.csv`, this file contains information about different airports all over the world."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ManicLzC1WI",
        "outputId": "eace0eb3-d90d-48a7-a1d4-0a2122d7f9fd"
      },
      "source": [
        "file_path = MAIN_DIRECTORY +'/dataset/airports.csv'\n",
        "df_airports = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "df_airports.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+------------+----+---+---+\n",
            "|faa|                name|       lat|         lon| alt| tz|dst|\n",
            "+---+--------------------+----------+------------+----+---+---+\n",
            "|04G|   Lansdowne Airport|41.1304722| -80.6195833|1044| -5|  A|\n",
            "|06A|Moton Field Munic...|32.4605722| -85.6800278| 264| -5|  A|\n",
            "|06C| Schaumburg Regional|41.9893408| -88.1012428| 801| -6|  A|\n",
            "|06N|     Randall Airport| 41.431912| -74.3915611| 523| -5|  A|\n",
            "|09J|Jekyll Island Air...|31.0744722| -81.4277778|  11| -4|  A|\n",
            "|0A9|Elizabethton Muni...|36.3712222| -82.1734167|1593| -4|  A|\n",
            "|0G6|Williams County A...|41.4673056| -84.5067778| 730| -5|  A|\n",
            "|0G7|Finger Lakes Regi...|42.8835647| -76.7812318| 492| -5|  A|\n",
            "|0P2|Shoestring Aviati...|39.7948244| -76.6471914|1000| -5|  U|\n",
            "|0S9|Jefferson County ...|48.0538086|-122.8106436| 108| -8|  A|\n",
            "+---+--------------------+----------+------------+----+---+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLMAwIv6C1WK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MluY_u0iC1WL"
      },
      "source": [
        "Let's look at performing column-wise operations. In Apache Spark, you can do this using the `.withColumn(colName, col)`  which returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
        "\n",
        "*Parameters*:  \n",
        "- **colName** – string, name of the new column.\n",
        "- **col** – a Column expression for the new column.\n",
        "\n",
        "The new `column` must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using `df.colName`.\n",
        "Apache Spark DataFrame is **immutable**. Immutable means that it can't be changed, and so columns can't be updated in place.\n",
        "For example:\n",
        "```python\n",
        "df = df.withColumn(\"newCol\", df.oldCol + 1)\n",
        "```\n",
        "The above code creates a DataFrame with the same columns as df plus a new column, `newCol`, where every entry is equal to the corresponding entry from `oldCol`, plus one.\n",
        "\n",
        "Sometimes we have to change a column data type to another one, in this case, we can use the following code:\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "df_name = df_name.withColumn(\"columnName\", col(\"columnName\").cast(\"DataType\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX_z9avxC1WO"
      },
      "source": [
        "### Exercise 4: Update `flights` DataFrame to include a new column called `duration_hrs`, that contains the duration of each flight in hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "criyd6_HC1WP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72yPhZkIC1WQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3onKQ2e8C1WR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiHz5d-UC1WS"
      },
      "source": [
        "### Exercise 5: Write a query using the `.filter()` method to find all the flights that flew over 1000 miles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWaPgWrSC1WU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPjV84UgC1WV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOlvSjrVC1WW"
      },
      "source": [
        "### Exercise 6: Write a query using `.filter()` method, to only keep flights from SEA to PDX. This query should only return `tailnum`, `origin`, and `dest` columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTNaAHB4C1WY"
      },
      "source": [
        "# Solution 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkXe6kTdC1Wa"
      },
      "source": [
        "#Solution 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIkyqXYlC1Wc"
      },
      "source": [
        "We can perform column-wise operations using `.select()` method. When we select a column using the `df.colName` notation. In `.select()` method, we can perform any column operation and it will return the transformed column.\n",
        "For example, the following command returns a column of flight durations in hours instead of minutes.\n",
        "```python\n",
        "df_flights.select(df_flights.air_time/60)\n",
        "```\n",
        "We can use the `alias()` method to rename a column we've selected. The following example shows how we can do that.\n",
        "```python\n",
        "df_flights.select((df_flights.air_time/60).alias(\"duration_hrs\")\n",
        "```\n",
        "If we want to stick to the SQL syntax, we can use `.selectExpr()` method as well. The following commad is equivalent to the previous code.\n",
        "\n",
        "```python\n",
        "df_flights.selectExpr(\"air_time/60 as duration_hrs\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkT1eO98C1We"
      },
      "source": [
        "### Exercise 7: Write a query that return these columns, `origin`, `dest`, `tailnum`, and average speed in KM per hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSC5I5sXC1Wf"
      },
      "source": [
        "#Solution 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZqmxEoqC1Wg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwQpWXnKC1Wi"
      },
      "source": [
        "#Solution 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ENRVJbdC1Wj"
      },
      "source": [
        "### Exercise 8: Find the the shortest (in terms of distance) flight that left PDX by first filtering and using the `.min()` method. Perform the filtering by referencing the column directly, not passing a SQL string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg9dismiC1Wl"
      },
      "source": [
        "#Solution 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBmjGd8WC1Wm"
      },
      "source": [
        "#Solutin 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYrfjFQnC1Wo"
      },
      "source": [
        "# Solution 3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt0WGWNAC1Wp"
      },
      "source": [
        "### Exercise 9: Find the the longest (in terms of time) flight that left SEA by filtering and using the `.max()` method. Perform the filtering by referencing the column directly, not passing a SQL string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFayQcTtC1Wq"
      },
      "source": [
        "if we run the following code, we will get an error, because `air_time` data type is string, first we should cast it to an integer column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX0pHhQeC1Wr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXHEXytsC1Wt"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdBz5CucC1Wv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTgyAig9C1Ww"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kkw3FBBC1Wy"
      },
      "source": [
        "#Solution 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_1M6FT6C1W0"
      },
      "source": [
        "#Solution 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pekSEkDPC1W1"
      },
      "source": [
        "### Exercise 10: Write a query that uses the `.avg()` method to get the average air time of Delta Airlines flights ( the carrier column value is \"DL\") that left SEA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZMPilE0C1W2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvfZn9mDC1W3"
      },
      "source": [
        "### Exercise 11: Write a query that uses the `.sum()` method to get the total number of hours all planes spent in the air by creating a column called `duration_hrs` from the column `air_time`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgmqgHGBC1W5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gv87pWUC1W5"
      },
      "source": [
        "### Exercise 12: Write a query that uses `tailnum` column to count the number of flights each plane made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx94-3H4C1W7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXVjKrgoC1Xl"
      },
      "source": [
        "### Exercise 13: Write a query that returns the average duration of flights from `PDX` and `SEA`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiROq_AUC1Xm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXXsfv6iC1Xo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kKtTfULC1Xr"
      },
      "source": [
        "### Exercise 14: Write a query that returns the average departure delay (`dep_delay`) in each month for each destination. Then import PySpark functions to calculate the standard deviation of `dep_delay` by using `stddev()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1IKzZV2C1Xv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PooOVw7jC1Xw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFiR-fCJC1Xz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPzgZ6_JC1X5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCuypNh3C1X7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfobLNIfC1X8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vGNQ0QFC1X9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mos4EPquC1YB"
      },
      "source": [
        "### Exercise 15: Write a query that performs left outer join on the flights and airports DataFrames.\n",
        "- The flights and airports DataFrames are already in the workspace.\n",
        "- First, examine the airports DataFrame by calling .show() method.\n",
        "- Note which key column will let you join these two DataFrames.\n",
        "- Before joining these two DataFrames, rename the `faa` column in `airports` to `dest`, and then convert this DataFrame to a temporary view (table).\n",
        "- Use `spark.sql` to perform left outer join on these two tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jI3b44OC1YD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3HDLCuOC1YU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjUcQnD9C1YV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DN2yUVFC1Ya"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK_7Kb94C1Yb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQYQImhC1Yc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VB3tw29C1Yd"
      },
      "source": [
        "### Exercise 16: Rewrite the previous query by using DataFrame API `.join` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vi3e7G_C1Ye"
      },
      "source": [
        "In PySpark, we can use `.join` method to perform joins. This method takes three arguments.\n",
        "- The first argument is the second DataFrame that we want to join with the first one.\n",
        "- The second argument, `on`, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table.\n",
        "- The third argument, `how`, specifies the kind of join to perform.\n",
        "\n",
        "To perform left outer join set the value of `how` to `\"leftouter\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di9l8VerC1Yg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxQyAm7pC1Yh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-3vUb2mC1Yi"
      },
      "source": [
        "#### Awesome"
      ]
    }
  ]
}